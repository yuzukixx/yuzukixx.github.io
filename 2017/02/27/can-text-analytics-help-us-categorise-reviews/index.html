<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Can text analytics help us categorise reviews?</title>
  <meta name="description" content="Text mining is sort of a more specialised area in analytics, so here is the writeup of an assignment I did in Python that I think is a good brief introduction to it. For this assignment, we are trying to figure out if the content of a review in the Google Play Store can help us discern what category of app the review is for.">
  <meta name="keywords" content="data, analytics, text, categorisation">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://yuzukixx.github.io/2017/02/27/can-text-analytics-help-us-categorise-reviews/">
  
  
  <link rel="alternate" type="application/rss+xml" title=".. and maybe some interpolation" href="https://yuzukixx.github.io/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Can text analytics help us categorise reviews?">
  <meta name="twitter:description" content="Text mining is sort of a more specialised area in analytics, so here is the writeup of an assignment I did in Python that I think is a good brief introduction to it. For this assignment, we are try...">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-92825184-1', 'auto');
    ga('send', 'pageview');

  </script>


  
    <meta name="google-site-verification" content="7z8fHKNsmBfEIrFFoXpjquX3MACSp1OHBEljyJdWhuM">
  
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">.. and maybe some interpolation</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">// About</a>
      
        
        <a class="page-link" href="/archives/">// Archives</a>
      
        
        <a class="page-link" href="/contact/">// Contact</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Can text analytics help us categorise reviews?</h1>
    
    <p class="post-meta"><time datetime="Feb 27, 2017" itemprop="datePublished">Feb 27, 2017</time> • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">yuzukixx</span></span> • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/python/">python</a>,
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/data/">data</a>,
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/text-analytics/">text analytics</a>
    
  

</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Text mining is sort of a more specialised area in analytics, so here is the writeup of an assignment I did in Python that I think is a good brief introduction to it. For this assignment, we are trying to figure out if the content of a review in the Google Play Store can help us discern what category of app the review is for.
<!-- more --></p>

<p><br /></p>

<h2>The data</h2>
<p>The dataset I used is unavailable online, but it contains reviews from 500 apps, 100 from each of the following categories: social, education, finance, game, and weather. After some wrangling of the CSV files, I managed to combine them all in one large CSV with just 2 columns.</p>

<p><br /></p>

<p><img src="../../../../assets/images/27022017_data1.png" alt="Sample of data file" /></p>

<p><br />
We’ll refer to the left column as just the ‘review’ column. Each review is represented there as a string that combines both the title of the review and its actual content. On the right is the category of the app this review was for.</p>

<p><br /></p>

<h2>Basic steps</h2>
<ol>
  <li>Import all the libraries/functions needed.
    <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
</code></pre>
    </div>
  </li>
  <li>Read in the data as a pandas dataframe.
    <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'allReviews.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'reviews'</span><span class="p">,</span> <span class="s">'category'</span><span class="p">]</span>
</code></pre>
    </div>
  </li>
  <li>Split the data into training and testing sets. I used a 70%-30% split.
    <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">rd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.7</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">rd</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">rd</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'category'</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s">'category'</span><span class="p">]</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'reviews'</span><span class="p">]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s">'reviews'</span><span class="p">]</span>
</code></pre>
    </div>
  </li>
  <li>
    <p>Preprocess the textual data and create a sparse feature matrix from it using scikit-learn’s TfidfVectorizer.</p>

    <p>TfidfVectorizer does most of the basic text preprocessing steps for us e.g. conversion to lowercase and filtering stop words, as well as turning it into the sparse feature matrix of tokens we will use as input. However, it is not easy to introduce stemming into the process (at least at the time of writing).</p>

    <div class="language-python highlighter-rouge"><pre class="highlight"><code> <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">strip_accents</span> <span class="o">=</span> <span class="s">'ascii'</span><span class="p">,</span> <span class="n">decode_error</span> <span class="o">=</span> <span class="s">'ignore'</span><span class="p">,</span> <span class="n">stop_words</span> <span class="o">=</span> <span class="s">'english'</span><span class="p">)</span>
 <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</code></pre>
    </div>
  </li>
  <li>Train a classifier using our training data. I chose to use naive Bayes because of its speed.
    <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre>
    </div>
  </li>
  <li>Transform the testing data into a feature matrix and use our previously trained classifier to predict their category.
    <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">predictTfidf</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predictTfidf</span><span class="p">)</span>
</code></pre>
    </div>
  </li>
  <li>Output the results.
    <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">cnf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">((</span><span class="n">cnf_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">cnf_matrix</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">cnf_matrix</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">cnf_matrix</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">cnf_matrix</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span><span class="o">/</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre>
    </div>
  </li>
</ol>

<p><br /></p>

<p>I also adapted some code from <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py">here</a> to print out a nice confusion matrix.</p>

<p><img src="../../../../assets/images/27022017_cm1.png" alt="Confusion matrices (all data)" /></p>

<p>The accuracy here is about 68%. Much better than the expected 20% accuracy for random guessing, but we can do better.</p>

<p><br /></p>

<h2>Refining the model</h2>

<h3>Removing non-English characters/less significant data points</h3>
<p>The vast majority of reviews appear to be in English, so it would simplify things without losing too much information if we filtered out all non-alphanumeric characters (including emojis).</p>

<p>In addition, we should filter out reviews which are too short because for the most part, these reviews do not have enough information to be classified. As an example, consider a review which simply says ‘Great app.’ It is equally applicable to any of the categories and as such not particularly useful in training or testing our classifier.</p>

<p>We can consider review length in 2 ways: by its word count or by its character count. Counting the number of words is theoretically possible by using the nltk package’s word_tokenize function, but it slows down the processing a lot. Using a character count is much faster, with basically the same benefit, so that’s what I used. I ran the code 3 times, filtering reviews with less than 10 characters, 50 characters, and 100 characters respectively.</p>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Cut-off point chosen</th>
      <th> </th>
      <th>10 characters</th>
      <th> </th>
      <th>50 characters</th>
      <th> </th>
      <th>100 characters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Number of reviews remaining</td>
      <td> </td>
      <td>1 326 290</td>
      <td> </td>
      <td>655 355</td>
      <td> </td>
      <td>339 078</td>
    </tr>
    <tr>
      <td>Accuracy</td>
      <td> </td>
      <td>71%</td>
      <td> </td>
      <td>79.6%</td>
      <td> </td>
      <td>82%</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p><strong>Note</strong>: Even with the exact dataset I used, the numbers above will vary every time because I redo the random splitting of dataset. But with a dataset this size, the approximate value is always about the same.</p>

<p>As seen from the table above, the accuracy of the model generally increases when we take longer reviews, which is the expected result. Finance is the category that is best predicted, followed by social, game, then weather and education.</p>

<p>There is a large jump in accuracy between taking reviews that have at least 10 characters and those that have at least 50, but a much smaller one between taking reviews that have at least 50 characters and reviews that have at least 100. Given that it is more useful for a classifier to be able to classify a wider range of reviews, it is reasonable to trade off some accuracy for the ability to classify more reviews. From this point on, we will use only reviews with at least 50 characters.</p>

<p><br /></p>

<h3>N-grams</h3>
<p>We can use the ngram_range parameter of TfidfVectorizer to take into account n-grams. Previously, each feature was a single word with 2 or more alphanumeric characters. In this case, I chose to try adding the parameter ngram_range = (1,2) to consider both single words and pairs of words.</p>

<p><img src="../../../../assets/images/27022017_cm2.png" alt="Confusion matrices (&gt;=50 characters, ngram_range = (1,2))" /></p>

<p>The above are the confusion matrices generated whien I ran the code once, using only reviews with more than 50 characters and also taking into account pairs of words. The accuracy was about 78.5%, not much different from when we only considered single words.</p>

<p>More importantly, taking into account pairs of words seems to increase the accuracy of categorising the finance and social app reviews (which have more data points) at the expense of the other 3 categories. Since it appears to slightly overvalue categories with more training data, we’ll drop this from consideration.</p>

<p><br /></p>

<h3>Number of features used</h3>
<p>Generally, when we create a feature matrix from our training data, we end up with slightly over 100 000 features used in our classifier. We can reduce the number of features that are used in training our classifier to prevent overfitting to our training data. This can be adjusted using the max_features parameter of TfidfVectorizer.</p>

<p>I ran the Python script 10 times, at intervals of 10 000 features, to try and get an idea of how the accuracy changes according to the number of features used.</p>

<p><img src="../../../../assets/images/27022017_graph1.png" alt="Accuracy against number of features" /></p>

<p>This is the graph of accuracy against the number of features used to train the classifier, with a best fit curve overlaid over the plot. It peaks at about 30 000 features.</p>

<p><br /></p>

<p><img src="../../../../assets/images/27022017_cm3.png" alt="Confusion matrices (&gt;=50 characters, 30 000 features" /></p>

<p>These are the confusion matrices generated, considering only reviews with at least 50 characters and using 30 000 features to train the classifier.</p>

<p>From the graph of accuracy against number of features, the absolute increase in accuracy is small (1% or less). However, the confusion matrices show that taking fewer features appears to make the model more robust by minimising the effects of the differences in amount of training data available for each category. Education, game and weather all have significantly more correct predictions, while finance and social have slightly less. Hence I chose to leave this in the script.</p>

<p><br /></p>

<h3>Using different classifiers</h3>
<p>Previously, I mentioned that I used naive Bayes as the main classifier because it was relatively fast and still quite accurate, which made it ideal for use while we tested and refined the model. However, other classifiers may do a better job, so I repeated the classification process (reviews &gt;= 50 characters, number of features used = 30 000) with random forest and linear support vector classifiers.</p>

<p><img src="../../../../assets/images/27022017_cm4.png" alt="Confusion matrices (Random Forest)" /></p>

<p>These are the confusion matrices generated for the random forest classifier (sklearn’s RandomForestClassifier). Its accuracy is about 76%.</p>

<p><br /></p>

<p><img src="../../../../assets/images/27022017_cm5.png" alt="Confusion matrices (Linear SVM)" /></p>

<p>This was the result of using a linear support vector classifier (sklearn’s LinearSVC). Its the most accurate of the 3, with an accuracy of about 82.5%. Interestingly, both random forest and linear support vector classifiers were much better at determining game reviews, as compared to naive Bayes which tended to have finance as its best predicted category.</p>

<p><br /></p>

<h2>Summary</h2>
<p>This is the outline of the final series of steps used:</p>

<ol>
  <li>Import all libraries needed and read in the data as a pandas dataframe.</li>
  <li>Remove all non-alphanumeric characters from the data.</li>
  <li>Remove all rows where the review column is less than 50 characters.</li>
  <li>Split the resulting dataset into training and test sets.</li>
  <li>Use TfidfVectorizer for preprocessing and creation of feature matrix.</li>
  <li>Feed input into a linear support vector classifier.</li>
  <li>Predict output for the testing data and print the results.</li>
</ol>

<p>The code for this process can be found <a href="https://github.com/yuzukixx/code-snippets/blob/master/gps_review_categorisation.py">here</a>.</p>

  </div>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; yuzukixx. Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a>.

    </p>

  </div>

</footer>


  </body>

</html>
